> **TODO:** Expand this guide with detailed content.

# Quantization

Reducing model precision to optimize inference.

- **Techniques:**
    - Post-Training Quantization (PTQ) vs Quantization-Aware Training (QAT).
    - FP16 vs INT8 vs INT4 vs BitsAndBytes (NF4).
    - GPTQ vs AWQ.
- **Tradeoffs:** Memory reduction vs Accuracy drop.
