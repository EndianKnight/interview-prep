> **TODO:** Expand this guide with detailed content.

# ML Basics

Core machine learning concepts, algorithms, and mathematical foundations — the building blocks for every AI engineering interview.

## Topics to Cover

### Learning Paradigms
- **Supervised learning** — regression, classification, labeled data
- **Unsupervised learning** — clustering, dimensionality reduction, anomaly detection
- **Semi-supervised** — small labeled + large unlabeled dataset
- **Self-supervised** — create labels from data itself (masked language modeling, contrastive)
- **Reinforcement learning** — agent, environment, reward, policy (RLHF context)

### Core Algorithms
- **Linear/Logistic Regression** — loss functions, gradient descent, regularization (L1/L2)
- **Decision Trees & Random Forests** — information gain, bagging, feature importance
- **Gradient Boosting** — XGBoost, LightGBM, CatBoost — sequential weak learners
- **SVMs** — kernel trick, margin maximization, support vectors
- **KNN** — distance metrics, curse of dimensionality
- **Naive Bayes** — Bayes theorem, conditional independence assumption

### Key Concepts
- **Bias-variance tradeoff** — underfitting vs overfitting, model complexity
- **Regularization** — L1 (Lasso, sparsity), L2 (Ridge, weight decay), dropout, early stopping
- **Cross-validation** — k-fold, stratified, leave-one-out
- **Feature engineering** — scaling, encoding (one-hot, label, target), feature selection
- **Imbalanced classes** — oversampling (SMOTE), undersampling, class weights, focal loss

### Optimization
- **Gradient descent** — batch, mini-batch, stochastic (SGD)
- **Optimizers** — Adam, AdamW, SGD with momentum, learning rate schedules
- **Loss functions** — MSE, cross-entropy, hinge loss, contrastive loss

### Interview Questions
- Explain bias-variance tradeoff
- When would you use L1 vs L2 regularization?
- How do you handle imbalanced datasets?
- Random Forest vs Gradient Boosting — when to use which?
- Explain gradient descent and why learning rate matters
