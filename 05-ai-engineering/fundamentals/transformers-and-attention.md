# Transformers & Attention

> TODO: Architecture, self-attention, multi-head attention, positional encoding
